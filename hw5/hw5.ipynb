{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS145 Howework 5\n",
    "\n",
    "\n",
    "<span style=\"color:red\"> **Important Note:** </span>\n",
    "HW4 is due on **11:59 PM PT, Dec 4 (Friday, Week 9)**. Please submit through GradeScope. \n",
    "\n",
    "## Print Out Your Name and UID\n",
    "\n",
    "<span style=\"color:blue\"> **Name: Kevin Li, UID: XXXXXXXXX** </span>\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "You need to first create HW5 conda environment by the given `cs145hw5.yml` file, which provides the name and necessary packages for this tasks. If you have `conda` properly installed, you may create, activate or deactivate by the following commands:\n",
    "\n",
    "```\n",
    "conda env create -f cs145hw5.yml\n",
    "conda activate hw4\n",
    "conda deactivate\n",
    "```\n",
    "OR \n",
    "\n",
    "```\n",
    "conda env create --name NAMEOFYOURCHOICE -f cs145hw5.yml \n",
    "conda activate NAMEOFYOURCHOICE\n",
    "conda deactivate\n",
    "```\n",
    "To view the list of your environments, use the following command:\n",
    "```\n",
    "conda env list\n",
    "```\n",
    "\n",
    "More useful information about managing environments can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "You may also quickly review the usage of basic Python and Numpy package, if needed in coding for matrix operations.\n",
    "\n",
    "In this notebook, you must not delete any code cells in this notebook. If you change any code outside the blocks (such as some important hyperparameters) that you are allowed to edit (between STRART/END YOUR CODE HERE), you need to highlight these changes. You may add some additional cells to help explain your results and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sys \n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import Digraph\n",
    "from IPython.display import Image\n",
    "from scipy.stats import multivariate_normal\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can successfully run the code above, there will be no problem for environment setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Frequent Pattern Mining for Set Data (25 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 1\n",
    "| TID | Items | \n",
    "| --- | --- | \n",
    "| 1 | b,c,j | \n",
    "| 2 | a,b,d |\n",
    "| 3 | a,c |\n",
    "| 4 | b,d | \n",
    "| 5 | a,b,c,e | \n",
    "| 6 | b,c,k| \n",
    "| 7 | a,c |\n",
    "| 8 | a,b,e,i | \n",
    "| 9 | b,d | \n",
    "| 10 | a,b,c,d |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Given a transaction database shown in Table 1, answer the following questions. Let the parameter `min_support` be 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Questions**\n",
    "\n",
    "### 1.1 Apriori Algorothm (16 pts) . \n",
    "<span style=\"color:red\"> Note: This is a \"question-answer\" style problem. You do not need to code anything and you are required to calculate by hand (with a scientific calculator). </span>\n",
    "Find all the frequent patterns using Apriori Algorithm. <br>\n",
    "a. $C_1$ <br>\n",
    "b. $L_1$<br>\n",
    "c. $C_2$<br>\n",
    "d. $L_2$<br>\n",
    "e. $C_3$<br>\n",
    "f. $L_3$<br>\n",
    "g. $C_4$ <br>\n",
    "h. $L_4$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "C1\n",
    "\n",
    "{'a',}: 6<br>\n",
    "{'b',}: 8<br>\n",
    "{'c',}: 6<br>\n",
    "{'d',}: 4<br>\n",
    "{'e',}: 2<br>\n",
    "{'i',}: 1<br>\n",
    "{'j',}: 1<br>\n",
    "{'k',}: 1<br>\n",
    "\n",
    "L1\n",
    "\n",
    "{'a',}: 6<br>\n",
    "{'b',}: 8<br>\n",
    "{'c',}: 6<br>\n",
    "{'d',}: 4<br>\n",
    "{'e',}: 2<br>\n",
    "\n",
    "==================================================\n",
    "\n",
    "C2\n",
    "\n",
    "{'a', 'b'}: 4<br>\n",
    "{'a', 'c'}: 4<br>\n",
    "{'a', 'd'}: 2<br>\n",
    "{'a', 'e'}: 2<br>\n",
    "{'b', 'c'}: 4<br>\n",
    "{'b', 'd'}: 4<br>\n",
    "{'b', 'e'}: 2<br>\n",
    "{'c', 'd'}: 1<br>\n",
    "{'c', 'e'}: 1<br>\n",
    "{'d', 'e'}: 0<br>\n",
    "\n",
    "L2\n",
    "\n",
    "{'a', 'b'}: 4<br>\n",
    "{'a', 'c'}: 4<br>\n",
    "{'a', 'd'}: 2<br>\n",
    "{'a', 'e'}: 2<br>\n",
    "{'b', 'c'}: 4<br>\n",
    "{'b', 'd'}: 4<br>\n",
    "{'b', 'e'}: 2<br>\n",
    "\n",
    "==================================================\n",
    "\n",
    "C3\n",
    "\n",
    "{'a', 'b', 'c'}: 2<br>\n",
    "{'a', 'b', 'd'}: 2<br>\n",
    "{'a', 'b', 'e'}: 2<br>\n",
    "\n",
    "L3\n",
    "\n",
    "{'a', 'b', 'c'}: 2<br>\n",
    "{'a', 'b', 'd'}: 2<br>\n",
    "{'a', 'b', 'e'}: 2<br>\n",
    "\n",
    "==================================================\n",
    "\n",
    "C4 is empty\n",
    "\n",
    "\n",
    "L4 is empty\n",
    "\n",
    "\n",
    "==================================================\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 FP-tree (9 pts)\n",
    "(a)Construct the FP-tree of the table.  \n",
    "(b) For the item d, show its conditional pattern base (projected database) and conditional FP-tree <br> \n",
    "You may use Package `graphviz` to generate graph (https://graphviz.readthedocs.io/en/stable/manual.html) (Bonus point: 5pts) or draw by hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Find frequent patterns based on d's conditional FP-tree <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from graphviz import Digraph\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "min_sup = 2\n",
    "db = [\n",
    "    ['b', 'c', 'j'],\n",
    "    ['a', 'b', 'd'],\n",
    "    ['a', 'c'],\n",
    "    ['b', 'd'],\n",
    "    ['a', 'b', 'c', 'e'],\n",
    "    ['b', 'c', 'k'],\n",
    "    ['a', 'c'],\n",
    "    ['a', 'b', 'e', 'i'],\n",
    "    ['b', 'd'],\n",
    "    ['a', 'b', 'c', 'd']\n",
    "]\n",
    "\n",
    "db_items = np.array(list(''.join(''.join(t) for t in db)))\n",
    "items, counts = np.unique(db_items, return_counts=True)\n",
    "item_data = sorted(zip(list(-counts), list(items)))\n",
    "_, item_order = zip(*item_data)\n",
    "frequents = [(item, -count) for count, item in item_data if -count >= min_sup]\n",
    "frequent_names, frequent_counts = list(zip(*frequents))\n",
    "\n",
    "class Node:\n",
    "    names = defaultdict(int)\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.value = 0\n",
    "        self.children = {}\n",
    "        self.hidden_name = name + str(Node.names[name])\n",
    "        Node.names[name] += 1\n",
    "\n",
    "    def add(self, item):\n",
    "        if item in self.children:\n",
    "            child = self.children[item]\n",
    "        else:\n",
    "            child = Node(item)\n",
    "            self.children[item] = child\n",
    "        child.value += 1\n",
    "        return child\n",
    "\n",
    "root = Node('{}')\n",
    "root.value = ''\n",
    "for tx in db:\n",
    "    n = root\n",
    "    for item in frequent_names:\n",
    "        if item in tx:\n",
    "            n = n.add(item)\n",
    "\n",
    "dot = Digraph()\n",
    "dot.node(root.hidden_name, f'{root.name}: {root.value}')\n",
    "\n",
    "frequent_nodes = defaultdict(list)\n",
    "fringe = deque([root])\n",
    "while fringe:\n",
    "    to_expand = fringe.popleft()\n",
    "    if to_expand.name in frequent_names:\n",
    "        frequent_nodes[to_expand.name].append(to_expand.hidden_name)\n",
    "\n",
    "    children = list(to_expand.children.values())\n",
    "    fringe.extend(children)\n",
    "    for child in children:\n",
    "        dot.node(child.hidden_name, f'{child.name}: {child.value}')\n",
    "        dot.edge(to_expand.hidden_name, child.hidden_name)\n",
    "\n",
    "table_label = '{ID|' + '|'.join(frequent_names) + '}|' + \\\n",
    "    '{Support Count|' + '|'.join(map(str, frequent_counts)) + '}|' + \\\n",
    "    '{Head|<' + '>|<'.join(frequent_names) + '>}'\n",
    "dot.node('table', label=table_label, shape='record')\n",
    "\n",
    "dot.attr('edge', style='dotted', constraint='false')\n",
    "between_edges = [[[item[i], item[i+1]] for i in range(len(item) - 1)] for item in frequent_nodes.values()]\n",
    "dot.edges([e for item_edges in between_edges for e in item_edges])\n",
    "dot.attr('edge', tailclip='false')\n",
    "head_edges = [[f'table:<{item}>:c', nodes[0]] for item, nodes in frequent_nodes.items()]\n",
    "dot.edges(head_edges)\n",
    "\n",
    "with open('graph1.dot', 'w') as f:\n",
    "    f.write(dot.source)\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graph1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dot=Digraph()\n",
    "dot.node('0','{}:')\n",
    "dot.node('1','b: 4')\n",
    "dot.node('2','a: 2')\n",
    "dot.edges([['0', '1'],['1', '2']])\n",
    "\n",
    "with open('graph2.dot', 'w') as f:\n",
    "    f.write(dot.source)\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graph2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "For part a, I wrote code to generate the fp-tree and header table based on setting min_support and db at the beginning\n",
    "\n",
    "for part b, I got the conditional pattern base by reading the fp-tree and moving up from each \"d\" node.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apriori for Yelp (50 pts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `apriori.py`, fill the missing lines. The parameters are set as `min_suppport=50` and `min_conf` = 0.25, and `ignore_one_iter_set=True`. Use the Yelp data `yelp.csv` and `id_nams.csv`, and run the following cell and report the frequent patterns and rules associated with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item: \n",
      "\"Holsteins Shakes & Buns\",\"Wicked Spoon\" 51\n",
      "item: \n",
      "\"Wicked Spoon\",\"Earl of Sandwich\" 52\n",
      "item: \n",
      "\"Secret Pizza\",\"Wicked Spoon\" 52\n",
      "item: \n",
      "\"Wicked Spoon\",\"The Cosmopolitan of Las Vegas\" 54\n",
      "item: \n",
      "\"Wicked Spoon\",\"Mon Ami Gabi\" 57\n",
      "item: \n",
      "\"Wicked Spoon\",\"Bacchanal Buffet\" 63\n",
      "\n",
      "------------------------ RULES:\n",
      "Rule: \n",
      "\"Secret Pizza\" \"Wicked Spoon\" 0.2561576354679803\n",
      "Rule: \n",
      "\"The Cosmopolitan of Las Vegas\" \"Wicked Spoon\" 0.27692307692307694\n",
      "Rule: \n",
      "\"Holsteins Shakes & Buns\" \"Wicked Spoon\" 0.3148148148148148\n"
     ]
    }
   ],
   "source": [
    "#No need to modify\n",
    "from hw5code.apriori import * \n",
    "input_file = read_data('./data/yelp.csv') \n",
    "min_support = 50\n",
    "min_conf = 0.25\n",
    "items, rules = run_apriori(input_file, min_support, min_conf)\n",
    "name_map = read_name_map('./data/id_name.csv')\n",
    "print_items_rules(items, rules, ignore_one_item_set=True, name_map=name_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these results mean? Do a quick Google search and briefly interpret the patterns and rules mined from Yelp in 50 words or less. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "We see that there we find patterns related Secret Pizza, Wicked Spoon, The Cosmopolitan of Las Vegas, and Holsteins Shakes and Buns.\n",
    "\n",
    "This makes a lot of sense upon searching them up, as all except Secret Pizza are part of the restaurant collection associated with The Cosmopolitan (a hotel in Las Vegas). Secret Pizza is also closely related, being a nearby restaurant. Since we're on Yelp, it is unsurprising to see these in close proximity because it is a review site, and people are likely to encounter these together and write about them together when reviewing locations.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis (10 pts)\n",
    "<span style=\"color:red\"> Note: This is a \"question-answer\" style problem. You do not need to code anything and you are required to calculate by hand (with a scientific calculator). </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 2\n",
    "| --- | Beer | No Beer | Total | \n",
    "| --- | --- | --- | --- | \n",
    "| Nuts | 150 | 700 | 850 | \n",
    "| No Nuts | 350 | 8800 | 9150 |\n",
    "| Total | 500 | 9500 | 10000 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 2 shows how many transactions containing beer and/or nuts among 10000 transactions.<br>\n",
    "\n",
    "\n",
    "Answer the following questions:<br>\n",
    "\n",
    "3.1 Calculate `confidence`,`lift` and `all_confidence` between buying beer and buying nuts. <br>\n",
    "3.2 What are you conclusions of the relationship between buying beer and buying nuts? Justify your conclusion with the previous measurements you calculated in 3.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "## 3.1\n",
    "\n",
    "confidence(beer -> nuts) = sup_count(beer, nuts) / sup_count(beer)\n",
    "confidence(beer -> nuts) = 150/850\n",
    "confidence(beer -> nuts) = 0.18\n",
    "\n",
    "confidence(nuts -> beer) = sup_count(nuts, beer) / sup_count(nuts)\n",
    "confidence(nuts -> beer) = 150/500\n",
    "confidence(nuts -> beer) = 0.30\n",
    "\n",
    "lift(beer, nuts) = P(beer, nuts) / (P(beer) * P(nuts))\n",
    "lift(beer, nuts) = (150/10000) / ((500/10000) * (850/10000))\n",
    "\n",
    "all_confidence(beer, nuts) = min(confidence(beer -> nuts), confidence(nuts -> beer))\n",
    "all_confidence(beer, nuts) = min(0.18, 0.30)\n",
    "all_confidence = 0.18\n",
    "\n",
    "## 3.2\n",
    "\n",
    "There exists a positive correlation between buying beer and nuts. We can see that the lift value is significantly over 1 (although there are a lot of no beer/no nut purchases which may bias it upwards) and that the confidence measures of beer -> nuts and nuts -> beer are much greater than the individual supports of nuts and beer (which are 0.05 and 0.085 respectively)\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sequential Pattern Mining (GSP Algorithm) (15 pts)                                                                                                                                                                                      \n",
    "<span style=\"color:red\"> Note: This is a \"question-answer\" style problem. You do not need to code anything and you are required to calculate by hand (with a scientific calculator). </span>\n",
    "\n",
    "4.1 For a sequence $s = <ab(cd)(ef)>$, how many events or elements does it contain? What is the length of $s$? How many non-empty subsequences does s contain? <br> \n",
    "4.2 Suppose we have $L_3 = \\{ <(ac)e> , <b(cd)> , <bce>, <a(cd)>, <(ab)d>, <(ab)c> \\}$, as the requent 3-sequences, write down all the candidate 4-sequences $C_4$ with the details of the join and pruning steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "## 4.1\n",
    "\n",
    "It contains 4 events. Its length is 6. It contains 2^6-1 = 63 non-empty subsequences\n",
    "\n",
    "## 4.2\n",
    "\n",
    "JOINING CANDIDATES:\n",
    "\n",
    "The joining table looks like this: (only the valid ones are filled in)\n",
    "A valid join is when dropping the first item from s1 is the same as dropping the second item from s2\n",
    "\n",
    "\n",
    "|         | <(ac)e> | <b(cd)>    | <bce>    | <a(cd)> | <(ab)d> | <(ab)c> |\n",
    "|---------|---------|------------|----------|---------|---------|---------|\n",
    "| <(ac)e> |         |            |          |         |         |         |\n",
    "| <b(cd)> |         |            |          |         |         |         |\n",
    "| <bce>   |         |            |          |         |         |         |\n",
    "| <a(cd)> |         |            |          |         |         |         |\n",
    "| <(ab)d> |         |            |          |         |         |         |\n",
    "| <(ab)c> |         | <(ab)(cd)> | <(ab)ce> |         |         |         |\n",
    "\n",
    "This gives us the two candidates <(ab)(cd)> and <(ab)ce>.\n",
    "\n",
    "PRUNING CANDIDATES:\n",
    "\n",
    "We prune by checking if each 3-length subseq of the candidates is in L3. This holds for <(ab)(cd)> but not for <(ab)ce> since no super sequence of <abe> is in L3.\n",
    "\n",
    "Thus, we end up with the following:\n",
    "\n",
    "C4 = <(ab)(cd)>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Bonus Question (10 pts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.In FP-tree, what will happen if we use `ascending` instead `descending` in header table? <br>\n",
    "2.Describe CloSpan (`Mining closed sequential patterns: CloSpan (Yan, Han & Afshar @SDMâ€™03)`). Compare with algorithms we discussed in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "## 5.1\n",
    "\n",
    "We use descending order to make the more frequent items higher and the less frequent items leaves. If we switch this order, we would build it with frequent items as the leaves and less frequent ones near the root. This would make FP-tree inefficient, since the point is that pattern bases are small and efficient because as you approach the leaves, the frequency get lower.\n",
    "\n",
    "## 5.2\n",
    "\n",
    "CloSpan is an attempt to improve PrefixSpan. To that end, it does the same general procedure of finding frequent items, projecting them onto the database and recursing on the projected database. The difference is in the pruning of the search space to seach faster and more effectively (e.g. using techniques like looking for common prefixes).\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Homework 5 :)\n",
    "After you've finished the homework, please print out the entire `ipynb` notebook and four `py` files into one PDF file. Make sure you include the output of code cells and answers for questions. Prepare submit it to GradeScope. Also this time remember assign the pages to the questions on GradeScope"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
