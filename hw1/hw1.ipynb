{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS145 Howework 1 \n",
    "\n",
    "<span style=\"color:red\"> **Important Note:** </span>\n",
    "HW1 is due on **11:59 PM PT, Oct 19 (Monday, Week 3)**. Please submit through GradeScope (you will receive an invite to Gradescope for CS145 Fall 2020.). \n",
    "\n",
    "## Print Out Your Name and UID\n",
    "\n",
    "<span style=\"color:blue\"> **Name: Kevin Li, UID: XXXXXXXXX** </span>\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "You need to first create HW1 conda environment by the given `cs145hw1.yml` file, which provides the name and necessary packages for this tasks. If you have `conda` properly installed, you may create, activate or deactivate by the following commands:\n",
    "\n",
    "```\n",
    "conda env create -f cs145hw1.yml\n",
    "conda activate hw1\n",
    "conda deactivate\n",
    "```\n",
    "OR \n",
    "\n",
    "```\n",
    "conda env create --name NAMEOFYOURCHOICE -f cs145hw1.yml \n",
    "conda activate NAMEOFYOURCHOICE\n",
    "conda deactivate\n",
    "```\n",
    "To view the list of your environments, use the following command:\n",
    "```\n",
    "conda env list\n",
    "```\n",
    "\n",
    "More useful information about managing environments can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "You may also quickly review the usage of basic Python and Numpy package, if needed in coding for matrix operations.\n",
    "\n",
    "In this notebook, you must not delete any code cells in this notebook. If you change any code outside the blocks that you are allowed to edit (between `STRART/END YOUR CODE HERE`), you need to highlight these changes. You may add some additional cells to help explain your results and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sys \n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can successfully run the code above, there will be no problem for environment setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear regression \n",
    "This workbook will walk you through a linear regression example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (1000, 100)\n",
      "Training labels shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "from hw1code.linear_regression import LinearRegression\n",
    "\n",
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "# As a sanity check, we print out the size of the training data (1000, 100) and training labels (1000,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Closed form solution\n",
    "In this section, complete the `getBeta` function in `linear_regression.py` which use the close for solution of $\\hat{\\beta}$.\n",
    "\n",
    "Train you model by using `lm.train('0')` function.\n",
    "\n",
    "Print the training error and the testing error using `lm.predict` and `lm.compute_mse` given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  0\n",
      "Training error is:  0.08693886675396781\n",
      "Testing error is:  0.11017540281675806\n"
     ]
    }
   ],
   "source": [
    "from hw1code.linear_regression import LinearRegression\n",
    "\n",
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "beta = lm.train('0')\n",
    "y_train_hat = lm.predict(lm.train_x, beta)\n",
    "y_test_hat = lm.predict(lm.test_x, beta)\n",
    "\n",
    "training_error = lm.compute_mse(y_train_hat, lm.train_y)\n",
    "testing_error = lm.compute_mse(y_test_hat, lm.test_y)\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training error is: ', training_error)\n",
    "print('Testing error is: ', testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Batch gradient descent\n",
    "In this section, complete the `getBetaBatchGradient` function in `linear_regression.py` which compute the gradient of the objective fuction. \n",
    "\n",
    "Train you model by using `lm.train('1')` function.\n",
    "\n",
    "Print the training error and the testing error using `lm.predict` and `lm.compute_mse` given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  1\n",
      "Training accuracy is:  0.08693902047792586\n",
      "Testing accuracy is:  0.11018845441523527\n"
     ]
    }
   ],
   "source": [
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "beta = lm.train('1')\n",
    "y_train_hat = lm.predict(lm.train_x, beta)\n",
    "y_test_hat = lm.predict(lm.test_x, beta)\n",
    "\n",
    "training_error = lm.compute_mse(y_train_hat, lm.train_y)\n",
    "testing_error = lm.compute_mse(y_test_hat, lm.test_y)\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_error)\n",
    "print('Testing accuracy is: ', testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Stochastic gadient descent \n",
    "In this section, complete the `getBetaStochasticGradient` function in `linear_regression.py`, which use an estimated gradient of the objective function.\n",
    "\n",
    "Train you model by using `lm.train('2')` function.\n",
    "\n",
    "Print the training error and the testing error using `lm.predict` and `lm.compute_mse` given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  2\n",
      "Training accuracy is:  0.09531517836589815\n",
      "Testing accuracy is:  0.11959236638904801\n"
     ]
    }
   ],
   "source": [
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "lm.lr = 0.0005\n",
    "beta = lm.train('2')\n",
    "y_train_hat = lm.predict(lm.train_x, beta)\n",
    "y_test_hat = lm.predict(lm.test_x, beta)\n",
    "\n",
    "training_error = lm.compute_mse(y_train_hat, lm.train_y)\n",
    "testing_error = lm.compute_mse(y_test_hat, lm.test_y)\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_error)\n",
    "print('Testing accuracy is: ', testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "1. Compare the MSE on the testing dataset for each version. Are they the same? Why or why not?\n",
    "2. Apply z-score normalization for eachh featrure and comment whether or not it affect the three algorithm. \n",
    "3. Ridge regression is adding an L2 regularization term to the original objective function of mean squared error. The objective function become following: \n",
    "    $$ J(\\beta) = \\frac{1}{2n} \\sum_i \\left(x_i^T\\beta - y_i \\right)^2 + \\frac{\\lambda}{2n} \\sum_j \\beta_j^2 ,$$ \n",
    "where $\\lambda \\geq 0$, which is a hyper parameter that controls the trade off. Take the derivative of this provided objective function and derive the closed form solution for $\\beta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature average std: 2.3522292509877913\n",
      "\n",
      "Learning Algorithm Type:  0\n",
      "Training error is:  0.08693886675396784\n",
      "Testing error is:  0.11017540281675804 \n",
      "\n",
      "Learning Algorithm Type:  1\n",
      "Training error is:  0.10018132194244624\n",
      "Testing error is:  0.13900607116647945 \n",
      "\n",
      "Learning Algorithm Type:  2\n",
      "Training error is:  0.10992817207625191\n",
      "Testing error is:  0.13117156993305695 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running normalized versions\n",
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "print(f'Feature average std: {lm.train_x.describe().loc[\"std\"].mean()}\\n')\n",
    "lm.normalize()\n",
    "\n",
    "def run_alg(a):\n",
    "    beta = lm.train(a)\n",
    "    training_error = lm.compute_mse(lm.predict(lm.train_x, beta), lm.train_y)\n",
    "    testing_error = lm.compute_mse(lm.predict(lm.test_x, beta), lm.test_y)\n",
    "    print('Training error is: ', training_error)\n",
    "    print('Testing error is: ', testing_error, '\\n')\n",
    "\n",
    "for a in ('0', '1', '2'):\n",
    "    run_alg(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "1. The MSE is slightly different but close for each version. They're different because they each take different paths to optimizing beta, but they all end up in a similar place because (it seems like) this problem is probably convex i.e. only has one optimum solution. To be precise, the first closed form solution should actually get the beta with the minimum error while BGD and SGD simply run for a certain number of iterations, then stop wherever they are. Since they involve choosing a random beta and SGD randomly samples from the training dataset, they will perform slightly randomly (although asymptotically they should still converge on the optimal beta)\n",
    "\n",
    "2. Z-scoring the features has no effect on the closed form solution but does slightly affect the gradient descent algorithms. This is because linear regression simply assigns a weight ($\\beta_i$) to each feature; if the feature's size is scaled, then the respective weight will be scaled appropriately to compensate. The \"zeroing\" of the features is also accounted for due to the addition of the bias term (which we see is added via the addAllOneColumn function). For the gradient descent algorithms, since they actually use the feature values to calculate their gradients and are affected by the feature values, we see mild effects on the model. One reason this happens is because Z-scoring (normalizing) is done to make each feature \"just as important\" as each other to avoid quickly converging on a solution that only uses some features. For example, if the standard deviation of $x_1$ is massive and the standard deviation of $x_2$ is tiny and we had the true solution be $y = x_1 + x_2$, then we would be able to converge on a \"pretty good\" solution by simply guessing $y = x_1$ i.e. $b_1 = 1, b_2 = 0$. Note that this does not neccessarily mean normalization improves performance! In fact in our case, it seems to hurt performance (although it would be nice to do something more like k-fold validation to investigate more thoroughly). It also helps with avoiding overflow which we saw affect SGD (previously without normalization, we needed to reduce the learning rate to avoid overflow).\n",
    "\n",
    "3. The solution ends up being $\\beta = (X^TX+\\lambda I)^{-1} X^T y$\n",
    "\n",
    "$$ J(\\beta) = \\frac{1}{2n} \\sum_i \\left(x_i^T\\beta - y_i \\right)^2 + \\frac{\\lambda}{2n} \\sum_j \\beta_j^2 $$ \n",
    "$$ J(\\beta) = \\frac{1}{2n} (X\\beta - Y)^T(X\\beta - Y) + \\frac{\\lambda}{2n} \\beta^T \\beta $$\n",
    "We know what the left side (original loss) turns into since it's the same as the original loss function $J(\\beta)$, so we get the following\n",
    "$$ \\frac{\\partial J}{\\partial \\beta} =  (X^TX\\beta - X^Ty) / n + \\frac{\\partial J}{\\partial \\beta} \\left( \\frac{\\lambda}{2n} \\beta^T \\beta \\right) $$\n",
    "Taking the derviative of the right side, we get\n",
    "$$ \\frac{\\partial J}{\\partial \\beta} =  (X^TX\\beta - X^Ty) / n + \\frac{\\lambda}{n} \\beta $$\n",
    "We set the derviative equal to zero and do some rearranging and get\n",
    "$$ 0 = \\frac{1}{n} \\left(X^TX\\beta - X^Ty + \\lambda \\beta \\right) $$\n",
    "\n",
    "$$ X^Ty = X^TX\\beta + \\lambda \\beta $$\n",
    "$$ X^Ty = \\beta (X^TX + \\lambda I) $$\n",
    "$$ (X^TX + \\lambda I)^{-1} X^Ty = \\beta $$\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic regression \n",
    "This workbook will walk you through a logistic regression example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (1000, 5)\n",
      "Training labels shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "from hw1code.logistic_regression import LogisticRegression\n",
    "\n",
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "# As a sanity chech, we print out the size of the training data (1000, 5) and training labels (1000,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Batch gradiend descent\n",
    "In this section, complete the `getBeta_BatchGradient` in `logistic_regression.py`, which compute the gradient of the log likelihoood function. \n",
    "\n",
    "Complete the `compute_avglogL` function in `logistic_regression.py` for sanity check. \n",
    "\n",
    "Train you model by using `lm.train('0')` function.\n",
    "\n",
    "And print the training and testing accuracy using `lm.predict` and `lm.compute_accuracy` given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average logL for iteration 0: -0.4893882425713696 \t\n",
      "average logL for iteration 1000: -0.460100375350853 \t\n",
      "average logL for iteration 2000: -0.460100375350853 \t\n",
      "average logL for iteration 3000: -0.460100375350853 \t\n",
      "average logL for iteration 4000: -0.460100375350853 \t\n",
      "average logL for iteration 5000: -0.460100375350853 \t\n",
      "average logL for iteration 6000: -0.460100375350853 \t\n",
      "average logL for iteration 7000: -0.460100375350853 \t\n",
      "average logL for iteration 8000: -0.460100375350853 \t\n",
      "average logL for iteration 9000: -0.460100375350853 \t\n",
      "Training avgLogL:  -0.460100375350853\n",
      "Training accuracy is:  0.797\n",
      "Testing accuracy is:  0.7534791252485089\n"
     ]
    }
   ],
   "source": [
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "training_accuracy= 0\n",
    "testing_accuracy= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "lm.normalize()\n",
    "\n",
    "beta = lm.train('0')\n",
    "y_train_hat = lm.predict(lm.train_x, beta)\n",
    "y_test_hat = lm.predict(lm.test_x, beta)\n",
    "\n",
    "training_accuracy = lm.compute_accuracy(y_train_hat, lm.train_y)\n",
    "testing_accuracy = lm.compute_accuracy(y_test_hat, lm.test_y)\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)\n",
    "print('Testing accuracy is: ', testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Newton Raphhson\n",
    "In this section, complete the `getBeta_Newton` in `logistic_regression.py`, which make use of both first and second derivative.\n",
    "\n",
    "Train you model by using `lm.train('1')` function.\n",
    "\n",
    "Print the training and testing accuracy using `lm.predict` and `lm.compute_accuracy` given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average logL for iteration 0: -0.4905626329298569 \t\n",
      "average logL for iteration 500: -0.460100375350853 \t\n",
      "average logL for iteration 1000: -0.460100375350853 \t\n",
      "average logL for iteration 1500: -0.460100375350853 \t\n",
      "average logL for iteration 2000: -0.460100375350853 \t\n",
      "average logL for iteration 2500: -0.460100375350853 \t\n",
      "average logL for iteration 3000: -0.460100375350853 \t\n",
      "average logL for iteration 3500: -0.460100375350853 \t\n",
      "average logL for iteration 4000: -0.460100375350853 \t\n",
      "average logL for iteration 4500: -0.460100375350853 \t\n",
      "average logL for iteration 5000: -0.460100375350853 \t\n",
      "average logL for iteration 5500: -0.460100375350853 \t\n",
      "average logL for iteration 6000: -0.460100375350853 \t\n",
      "average logL for iteration 6500: -0.460100375350853 \t\n",
      "average logL for iteration 7000: -0.460100375350853 \t\n",
      "average logL for iteration 7500: -0.460100375350853 \t\n",
      "average logL for iteration 8000: -0.460100375350853 \t\n",
      "average logL for iteration 8500: -0.460100375350853 \t\n",
      "average logL for iteration 9000: -0.460100375350853 \t\n",
      "average logL for iteration 9500: -0.460100375350853 \t\n",
      "Training avgLogL:  -0.460100375350853\n",
      "Training accuracy is:  0.797\n",
      "Testing accuracy is:  0.7534791252485089\n"
     ]
    }
   ],
   "source": [
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "training_accuracy= 0\n",
    "testing_accuracy= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "lm.normalize()\n",
    "\n",
    "beta = lm.train('1')\n",
    "y_train_hat = lm.predict(lm.train_x, beta)\n",
    "y_test_hat = lm.predict(lm.test_x, beta)\n",
    "\n",
    "training_accuracy = lm.compute_accuracy(y_train_hat, lm.train_y)\n",
    "testing_accuracy = lm.compute_accuracy(y_test_hat, lm.test_y)\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)\n",
    "print('Testing accuracy is: ', testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "1. Compare the accuracy on the testing dataset for each version. Are they the same? Why or why not?\n",
    "2. Regularization. Similar to linear regression, an regularization term could be added to logistic regression. \n",
    "The objective function becomes following: \n",
    "    $$ J(\\beta) = -\\frac{1}{n} \\sum_i \\left(y_i x_i^T \\beta - \\log \\left( 1+ \\exp\\{ x_i^T \\beta \\} \\right) \\right) + \\lambda \\sum_j \\beta_j^2,$$ \n",
    "where $\\lambda \\geq 0$, which is a hyper parameter that controls the trade off. Take the derivative $\\frac{\\partial J(\\beta)}{\\partial \\beta_j}$ of this provided objective function and provide the batch gradient descent update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "1. They are the same since the both converged on the same (probably very close to optimal) solution. Newton-Raphson is simply a different optimizer and can therefore have different properties (i.e. speed, ability to find global optimum, precision, etc) but if they both find the global optimum, they will obviously have the same results\n",
    "\n",
    "2. The batch gradient update ends up being $\\beta_j^{t + 1} = B_j^t + \\eta \\left(-2 \\lambda \\beta_j + \\frac{1}{n} \\sum_i x_{ij} (c_i - p_i(\\beta))\\right)$\n",
    "\n",
    "$$ J(\\beta) = -\\frac{1}{n} \\sum_i \\left(y_i x_i^T \\beta - \\log \\left( 1+ e^{x_i^T \\beta} \\right) \\right) + \\lambda \\sum_j \\beta_j^2$$\n",
    "Like before, we know the derivative of the left part of the equation since it's the regular log likelihood loss function and we covered the derivative in class, so we gradient\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\beta_j} = -\\frac{1}{n} \\sum_{i=1}^n x_{ij} (y_i - p_i(\\beta)) + \\frac{\\partial}{\\partial \\beta_j}\\lambda \\sum_j \\beta_j^2$$\n",
    "$$ \\frac{\\partial J}{\\partial \\beta_j} = -\\frac{1}{n} \\sum_{i=1}^n x_{ij} (y_i - p_i(\\beta)) + 2 \\lambda \\beta_j$$\n",
    "\n",
    "Now that we know the derivative, gradient descent becomes as simple as\n",
    "\n",
    "$$ \\beta_j^{t + 1} = B_j^t - \\eta \\frac{\\partial J}{\\partial \\beta_j} $$\n",
    "$$ \\beta_j^{t + 1} = B_j^t - \\eta \\left(-\\frac{1}{n} \\sum_i x_{ij} (y_i - p_i(\\beta)) + 2 \\lambda \\beta_j\\right) $$\n",
    "$$ \\beta_j^{t + 1} = B_j^t + \\eta \\left(-2 \\lambda \\beta_j + \\frac{1}{n} \\sum_i x_{ij} (y_i - p_i(\\beta))\\right) $$\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualize the decision boundary on a toy dataset\n",
    "\n",
    "In this subsection, you will use the same implementation for another small dataset with each datapoint $x$ with only two features $(x_1, x_2)$ to visualize the decision boundary of logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (99, 2)\n",
      "Training labels shape: (99,)\n"
     ]
    }
   ],
   "source": [
    "from hw1code.logistic_regression import LogisticRegression\n",
    "\n",
    "lm=LogisticRegression(verbose = False)\n",
    "lm.load_data('./data/logistic-regression-toy.csv','./data/logistic-regression-toy.csv')\n",
    "# As a sanity chech, we print out the size of the training data (99,2) and training labels (99,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block, you can apply the same implementation of logistic regression model (either in 2.1 or 2.2) to the toy dataset. Print out the $\\hat{\\beta}$ after training and accuracy on the train set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training avgLogL:  -0.329147431295712\n",
      "Beta: [-0.04717577  1.46005896  2.06586134]\n",
      "Training accuracy is:  0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "training_accuracy= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "lm.normalize()\n",
    "beta = lm.train('1')\n",
    "y_train_hat = lm.predict(lm.train_x, beta)\n",
    "y_test_hat = lm.predict(lm.test_x, beta)\n",
    "\n",
    "training_accuracy = lm.compute_accuracy(y_train_hat, lm.train_y)\n",
    "testing_accuracy = lm.compute_accuracy(y_test_hat, lm.test_y)\n",
    "\n",
    "print(f'Beta: {beta}')\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we try to plot the decision boundary of your learned logistic regression classifier. Generally, a decision boundary is the region of a space in which the output label of a classifier is ambiguous. That is, in the given toy data, given a datapoint $x=(x_1, x_2)$ on the decision boundary, the logistic regression classifier cannot decide whether $y=0$ or $y=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Is the decision boundary for logistic regression linear? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "Yes.\n",
    "\n",
    "We know the decision boundary for logistic regression is when $\\sigma(x^T \\beta) = 0.5$. Then, we know that\n",
    "\n",
    "$$ \\sigma(x^T \\beta) = 0.5 $$\n",
    "$$ \\frac{1}{1 + e^{-x^T \\beta}} = 0.5 $$\n",
    "$$ 1 = e^{-x^T \\beta} $$\n",
    "$$0 = -x^T \\beta $$\n",
    "\n",
    "This is the equation for a hyperplane - the decision boundary is linear\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the decision boundary in the following cell. Note that the code to plot the raw data points are given. You may need `plt.plot` function (see [here](https://matplotlib.org/tutorials/introductory/pyplot.html)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n",
       "<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 370.942187 248.518125\" width=\"370.942187pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       " <defs>\r\n",
       "  <style type=\"text/css\">\r\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\r\n",
       "  </style>\r\n",
       " </defs>\r\n",
       " <g id=\"figure_1\">\r\n",
       "  <g id=\"patch_1\">\r\n",
       "   <path d=\"M 0 248.518125 \r\n",
       "L 370.942187 248.518125 \r\n",
       "L 370.942187 0 \r\n",
       "L 0 0 \r\n",
       "z\r\n",
       "\" style=\"fill:none;\"/>\r\n",
       "  </g>\r\n",
       "  <g id=\"axes_1\">\r\n",
       "   <g id=\"patch_2\">\r\n",
       "    <path d=\"M 28.942188 224.64 \r\n",
       "L 363.742188 224.64 \r\n",
       "L 363.742188 7.2 \r\n",
       "L 28.942188 7.2 \r\n",
       "z\r\n",
       "\" style=\"fill:#ffffff;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"matplotlib.axis_1\">\r\n",
       "    <g id=\"xtick_1\">\r\n",
       "     <g id=\"line2d_1\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 0 0 \r\n",
       "L 0 3.5 \r\n",
       "\" id=\"mc6023fad55\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n",
       "      </defs>\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.160369\" xlink:href=\"#mc6023fad55\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_1\">\r\n",
       "      <!-- −2.0 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 10.59375 35.5 \r\n",
       "L 73.1875 35.5 \r\n",
       "L 73.1875 27.203125 \r\n",
       "L 10.59375 27.203125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-8722\"/>\r\n",
       "       <path d=\"M 19.1875 8.296875 \r\n",
       "L 53.609375 8.296875 \r\n",
       "L 53.609375 0 \r\n",
       "L 7.328125 0 \r\n",
       "L 7.328125 8.296875 \r\n",
       "Q 12.9375 14.109375 22.625 23.890625 \r\n",
       "Q 32.328125 33.6875 34.8125 36.53125 \r\n",
       "Q 39.546875 41.84375 41.421875 45.53125 \r\n",
       "Q 43.3125 49.21875 43.3125 52.78125 \r\n",
       "Q 43.3125 58.59375 39.234375 62.25 \r\n",
       "Q 35.15625 65.921875 28.609375 65.921875 \r\n",
       "Q 23.96875 65.921875 18.8125 64.3125 \r\n",
       "Q 13.671875 62.703125 7.8125 59.421875 \r\n",
       "L 7.8125 69.390625 \r\n",
       "Q 13.765625 71.78125 18.9375 73 \r\n",
       "Q 24.125 74.21875 28.421875 74.21875 \r\n",
       "Q 39.75 74.21875 46.484375 68.546875 \r\n",
       "Q 53.21875 62.890625 53.21875 53.421875 \r\n",
       "Q 53.21875 48.921875 51.53125 44.890625 \r\n",
       "Q 49.859375 40.875 45.40625 35.40625 \r\n",
       "Q 44.1875 33.984375 37.640625 27.21875 \r\n",
       "Q 31.109375 20.453125 19.1875 8.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-50\"/>\r\n",
       "       <path d=\"M 10.6875 12.40625 \r\n",
       "L 21 12.40625 \r\n",
       "L 21 0 \r\n",
       "L 10.6875 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-46\"/>\r\n",
       "       <path d=\"M 31.78125 66.40625 \r\n",
       "Q 24.171875 66.40625 20.328125 58.90625 \r\n",
       "Q 16.5 51.421875 16.5 36.375 \r\n",
       "Q 16.5 21.390625 20.328125 13.890625 \r\n",
       "Q 24.171875 6.390625 31.78125 6.390625 \r\n",
       "Q 39.453125 6.390625 43.28125 13.890625 \r\n",
       "Q 47.125 21.390625 47.125 36.375 \r\n",
       "Q 47.125 51.421875 43.28125 58.90625 \r\n",
       "Q 39.453125 66.40625 31.78125 66.40625 \r\n",
       "z\r\n",
       "M 31.78125 74.21875 \r\n",
       "Q 44.046875 74.21875 50.515625 64.515625 \r\n",
       "Q 56.984375 54.828125 56.984375 36.375 \r\n",
       "Q 56.984375 17.96875 50.515625 8.265625 \r\n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \r\n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \r\n",
       "Q 6.59375 17.96875 6.59375 36.375 \r\n",
       "Q 6.59375 54.828125 13.0625 64.515625 \r\n",
       "Q 19.53125 74.21875 31.78125 74.21875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-48\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(32.018963 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_2\">\r\n",
       "     <g id=\"line2d_2\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"82.205824\" xlink:href=\"#mc6023fad55\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_2\">\r\n",
       "      <!-- −1.5 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 12.40625 8.296875 \r\n",
       "L 28.515625 8.296875 \r\n",
       "L 28.515625 63.921875 \r\n",
       "L 10.984375 60.40625 \r\n",
       "L 10.984375 69.390625 \r\n",
       "L 28.421875 72.90625 \r\n",
       "L 38.28125 72.90625 \r\n",
       "L 38.28125 8.296875 \r\n",
       "L 54.390625 8.296875 \r\n",
       "L 54.390625 0 \r\n",
       "L 12.40625 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-49\"/>\r\n",
       "       <path d=\"M 10.796875 72.90625 \r\n",
       "L 49.515625 72.90625 \r\n",
       "L 49.515625 64.59375 \r\n",
       "L 19.828125 64.59375 \r\n",
       "L 19.828125 46.734375 \r\n",
       "Q 21.96875 47.46875 24.109375 47.828125 \r\n",
       "Q 26.265625 48.1875 28.421875 48.1875 \r\n",
       "Q 40.625 48.1875 47.75 41.5 \r\n",
       "Q 54.890625 34.8125 54.890625 23.390625 \r\n",
       "Q 54.890625 11.625 47.5625 5.09375 \r\n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \r\n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \r\n",
       "Q 12.796875 0.140625 7.71875 1.703125 \r\n",
       "L 7.71875 11.625 \r\n",
       "Q 12.109375 9.234375 16.796875 8.0625 \r\n",
       "Q 21.484375 6.890625 26.703125 6.890625 \r\n",
       "Q 35.15625 6.890625 40.078125 11.328125 \r\n",
       "Q 45.015625 15.765625 45.015625 23.390625 \r\n",
       "Q 45.015625 31 40.078125 35.4375 \r\n",
       "Q 35.15625 39.890625 26.703125 39.890625 \r\n",
       "Q 22.75 39.890625 18.8125 39.015625 \r\n",
       "Q 14.890625 38.140625 10.796875 36.28125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-53\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(70.064418 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_3\">\r\n",
       "     <g id=\"line2d_3\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"120.251278\" xlink:href=\"#mc6023fad55\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_3\">\r\n",
       "      <!-- −1.0 -->\r\n",
       "      <g transform=\"translate(108.109872 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_4\">\r\n",
       "     <g id=\"line2d_4\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"158.296733\" xlink:href=\"#mc6023fad55\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_4\">\r\n",
       "      <!-- −0.5 -->\r\n",
       "      <g transform=\"translate(146.155327 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_5\">\r\n",
       "     <g id=\"line2d_5\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"196.342188\" xlink:href=\"#mc6023fad55\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_5\">\r\n",
       "      <!-- 0.0 -->\r\n",
       "      <g transform=\"translate(188.390625 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_6\">\r\n",
       "     <g id=\"line2d_6\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"234.387642\" xlink:href=\"#mc6023fad55\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_6\">\r\n",
       "      <!-- 0.5 -->\r\n",
       "      <g transform=\"translate(226.43608 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_7\">\r\n",
       "     <g id=\"line2d_7\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"272.433097\" xlink:href=\"#mc6023fad55\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_7\">\r\n",
       "      <!-- 1.0 -->\r\n",
       "      <g transform=\"translate(264.481534 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_8\">\r\n",
       "     <g id=\"line2d_8\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"310.478551\" xlink:href=\"#mc6023fad55\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_8\">\r\n",
       "      <!-- 1.5 -->\r\n",
       "      <g transform=\"translate(302.526989 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_9\">\r\n",
       "     <g id=\"line2d_9\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"348.524006\" xlink:href=\"#mc6023fad55\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_9\">\r\n",
       "      <!-- 2.0 -->\r\n",
       "      <g transform=\"translate(340.572443 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"matplotlib.axis_2\">\r\n",
       "    <g id=\"ytick_1\">\r\n",
       "     <g id=\"line2d_10\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 0 0 \r\n",
       "L -3.5 0 \r\n",
       "\" id=\"m9dd25e0ef0\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n",
       "      </defs>\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.942188\" xlink:href=\"#m9dd25e0ef0\" y=\"211.437474\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_10\">\r\n",
       "      <!-- −2 -->\r\n",
       "      <g transform=\"translate(7.2 215.236692)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_2\">\r\n",
       "     <g id=\"line2d_11\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.942188\" xlink:href=\"#m9dd25e0ef0\" y=\"166.875099\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_11\">\r\n",
       "      <!-- −1 -->\r\n",
       "      <g transform=\"translate(7.2 170.674318)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_3\">\r\n",
       "     <g id=\"line2d_12\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.942188\" xlink:href=\"#m9dd25e0ef0\" y=\"122.312724\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_12\">\r\n",
       "      <!-- 0 -->\r\n",
       "      <g transform=\"translate(15.579688 126.111943)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_4\">\r\n",
       "     <g id=\"line2d_13\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.942188\" xlink:href=\"#m9dd25e0ef0\" y=\"77.750349\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_13\">\r\n",
       "      <!-- 1 -->\r\n",
       "      <g transform=\"translate(15.579688 81.549568)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_5\">\r\n",
       "     <g id=\"line2d_14\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.942188\" xlink:href=\"#m9dd25e0ef0\" y=\"33.187974\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_14\">\r\n",
       "      <!-- 2 -->\r\n",
       "      <g transform=\"translate(15.579688 36.987193)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"line2d_15\">\r\n",
       "    <defs>\r\n",
       "     <path d=\"M 0 3 \r\n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \r\n",
       "C 2.683901 1.55874 3 0.795609 3 0 \r\n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \r\n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \r\n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \r\n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \r\n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \r\n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \r\n",
       "z\r\n",
       "\" id=\"m715245087b\" style=\"stroke:#1f77b4;\"/>\r\n",
       "    </defs>\r\n",
       "    <g clip-path=\"url(#p1da5aa84c8)\">\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"94.103444\" xlink:href=\"#m715245087b\" y=\"147.123686\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"113.017381\" xlink:href=\"#m715245087b\" y=\"129.516759\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"79.959486\" xlink:href=\"#m715245087b\" y=\"161.215407\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"117.923983\" xlink:href=\"#m715245087b\" y=\"154.816457\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"79.579579\" xlink:href=\"#m715245087b\" y=\"147.68407\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"117.982828\" xlink:href=\"#m715245087b\" y=\"165.152789\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"124.388763\" xlink:href=\"#m715245087b\" y=\"167.20907\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"122.916935\" xlink:href=\"#m715245087b\" y=\"175.74714\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"75.296568\" xlink:href=\"#m715245087b\" y=\"174.445851\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"112.229067\" xlink:href=\"#m715245087b\" y=\"183.59803\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"164.842241\" xlink:href=\"#m715245087b\" y=\"200.567765\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"163.509364\" xlink:href=\"#m715245087b\" y=\"205.607848\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"156.135713\" xlink:href=\"#m715245087b\" y=\"214.756364\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"146.956918\" xlink:href=\"#m715245087b\" y=\"189.814108\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"149.684557\" xlink:href=\"#m715245087b\" y=\"181.982322\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"206.176327\" xlink:href=\"#m715245087b\" y=\"200.997649\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"196.660258\" xlink:href=\"#m715245087b\" y=\"166.810558\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"214.387869\" xlink:href=\"#m715245087b\" y=\"173.650533\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"231.148767\" xlink:href=\"#m715245087b\" y=\"198.926878\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"205.96394\" xlink:href=\"#m715245087b\" y=\"184.607089\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"138.634524\" xlink:href=\"#m715245087b\" y=\"154.19438\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"178.881816\" xlink:href=\"#m715245087b\" y=\"152.741869\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"174.228604\" xlink:href=\"#m715245087b\" y=\"159.393168\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"154.891973\" xlink:href=\"#m715245087b\" y=\"160.269755\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"164.076085\" xlink:href=\"#m715245087b\" y=\"154.893374\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"128.781576\" xlink:href=\"#m715245087b\" y=\"111.594646\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"119.929646\" xlink:href=\"#m715245087b\" y=\"109.359727\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"157.774897\" xlink:href=\"#m715245087b\" y=\"113.045877\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"120.984256\" xlink:href=\"#m715245087b\" y=\"109.117362\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"122.057792\" xlink:href=\"#m715245087b\" y=\"129.545636\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"75.567456\" xlink:href=\"#m715245087b\" y=\"127.271743\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"63.493738\" xlink:href=\"#m715245087b\" y=\"114.313705\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"97.408171\" xlink:href=\"#m715245087b\" y=\"132.505227\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"69.511331\" xlink:href=\"#m715245087b\" y=\"135.388042\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"78.657404\" xlink:href=\"#m715245087b\" y=\"151.131753\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"232.404015\" xlink:href=\"#m715245087b\" y=\"111.698398\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"211.547842\" xlink:href=\"#m715245087b\" y=\"77.696217\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"279.909743\" xlink:href=\"#m715245087b\" y=\"102.089086\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"255.577545\" xlink:href=\"#m715245087b\" y=\"181.92029\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"177.946229\" xlink:href=\"#m715245087b\" y=\"184.877002\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"148.979324\" xlink:href=\"#m715245087b\" y=\"136.830444\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"80.617422\" xlink:href=\"#m715245087b\" y=\"93.95813\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"186.056965\" xlink:href=\"#m715245087b\" y=\"133.134555\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"211.547842\" xlink:href=\"#m715245087b\" y=\"139.787156\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"187.215643\" xlink:href=\"#m715245087b\" y=\"116.133465\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"268.322983\" xlink:href=\"#m715245087b\" y=\"154.570712\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"238.1974\" xlink:href=\"#m715245087b\" y=\"151.614\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"298.448566\" xlink:href=\"#m715245087b\" y=\"187.094536\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"248.625487\" xlink:href=\"#m715245087b\" y=\"90.262241\"/>\r\n",
       "     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"107.266976\" xlink:href=\"#m715245087b\" y=\"99.132375\"/>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"line2d_16\">\r\n",
       "    <defs>\r\n",
       "     <path d=\"M 0 3 \r\n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \r\n",
       "C 2.683901 1.55874 3 0.795609 3 0 \r\n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \r\n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \r\n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \r\n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \r\n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \r\n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \r\n",
       "z\r\n",
       "\" id=\"mb1a8eed677\" style=\"stroke:#ff7f0e;\"/>\r\n",
       "    </defs>\r\n",
       "    <g clip-path=\"url(#p1da5aa84c8)\">\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"245.780668\" xlink:href=\"#mb1a8eed677\" y=\"82.874498\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"234.234196\" xlink:href=\"#mb1a8eed677\" y=\"72.817703\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"225.733002\" xlink:href=\"#mb1a8eed677\" y=\"86.434246\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"218.934842\" xlink:href=\"#mb1a8eed677\" y=\"82.247746\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"205.377136\" xlink:href=\"#mb1a8eed677\" y=\"91.549393\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"188.195433\" xlink:href=\"#mb1a8eed677\" y=\"84.357079\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"184.420579\" xlink:href=\"#mb1a8eed677\" y=\"80.338668\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"162.160129\" xlink:href=\"#mb1a8eed677\" y=\"75.913693\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"321.108973\" xlink:href=\"#mb1a8eed677\" y=\"122.310689\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"280.187903\" xlink:href=\"#mb1a8eed677\" y=\"119.730922\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"305.373258\" xlink:href=\"#mb1a8eed677\" y=\"116.628559\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"312.16453\" xlink:href=\"#mb1a8eed677\" y=\"106.905428\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"295.558684\" xlink:href=\"#mb1a8eed677\" y=\"117.192072\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"312.143821\" xlink:href=\"#mb1a8eed677\" y=\"76.172043\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"346.21465\" xlink:href=\"#mb1a8eed677\" y=\"72.909452\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"290.01068\" xlink:href=\"#mb1a8eed677\" y=\"52.405263\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"317.512811\" xlink:href=\"#mb1a8eed677\" y=\"81.620115\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"322.185659\" xlink:href=\"#mb1a8eed677\" y=\"57.464465\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"259.91347\" xlink:href=\"#mb1a8eed677\" y=\"105.896443\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"271.912483\" xlink:href=\"#mb1a8eed677\" y=\"119.027529\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"292.879136\" xlink:href=\"#mb1a8eed677\" y=\"107.419355\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"282.615801\" xlink:href=\"#mb1a8eed677\" y=\"114.507823\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"263.725296\" xlink:href=\"#mb1a8eed677\" y=\"104.08451\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"274.02649\" xlink:href=\"#mb1a8eed677\" y=\"43.274779\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"261.919105\" xlink:href=\"#mb1a8eed677\" y=\"35.906445\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"318.462688\" xlink:href=\"#mb1a8eed677\" y=\"119.990015\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"217.364804\" xlink:href=\"#mb1a8eed677\" y=\"92.252029\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"264.749694\" xlink:href=\"#mb1a8eed677\" y=\"80.472247\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"224.495138\" xlink:href=\"#mb1a8eed677\" y=\"64.258699\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"187.441602\" xlink:href=\"#mb1a8eed677\" y=\"56.354848\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"204.109231\" xlink:href=\"#mb1a8eed677\" y=\"49.073165\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"229.757841\" xlink:href=\"#mb1a8eed677\" y=\"43.873854\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"162.973091\" xlink:href=\"#mb1a8eed677\" y=\"42.726031\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"202.278432\" xlink:href=\"#mb1a8eed677\" y=\"190.051248\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"179.104907\" xlink:href=\"#mb1a8eed677\" y=\"161.223312\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"71.348011\" xlink:href=\"#mb1a8eed677\" y=\"82.870463\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"100.314918\" xlink:href=\"#mb1a8eed677\" y=\"17.083636\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"270.640334\" xlink:href=\"#mb1a8eed677\" y=\"166.397558\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"310.790426\" xlink:href=\"#mb1a8eed677\" y=\"134.570187\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"322.645028\" xlink:href=\"#mb1a8eed677\" y=\"139.260831\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"337.843558\" xlink:href=\"#mb1a8eed677\" y=\"157.527424\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"165.200791\" xlink:href=\"#mb1a8eed677\" y=\"102.828264\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"107.266976\" xlink:href=\"#mb1a8eed677\" y=\"100.61073\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"139.709911\" xlink:href=\"#mb1a8eed677\" y=\"135.352088\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"230.086664\" xlink:href=\"#mb1a8eed677\" y=\"110.95922\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"271.799012\" xlink:href=\"#mb1a8eed677\" y=\"130.917021\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"131.599179\" xlink:href=\"#mb1a8eed677\" y=\"63.65184\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"139.709911\" xlink:href=\"#mb1a8eed677\" y=\"77.696217\"/>\r\n",
       "     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"123.488443\" xlink:href=\"#mb1a8eed677\" y=\"79.174575\"/>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"line2d_17\">\r\n",
       "    <path clip-path=\"url(#p1da5aa84c8)\" d=\"M 44.160369 58.305692 \r\n",
       "L 348.524006 184.284514 \r\n",
       "\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_3\">\r\n",
       "    <path d=\"M 28.942188 224.64 \r\n",
       "L 28.942188 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_4\">\r\n",
       "    <path d=\"M 363.742188 224.64 \r\n",
       "L 363.742188 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_5\">\r\n",
       "    <path d=\"M 28.942188 224.64 \r\n",
       "L 363.742188 224.64 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_6\">\r\n",
       "    <path d=\"M 28.942188 7.2 \r\n",
       "L 363.742188 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "  </g>\r\n",
       " </g>\r\n",
       " <defs>\r\n",
       "  <clipPath id=\"p1da5aa84c8\">\r\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"28.942188\" y=\"7.2\"/>\r\n",
       "  </clipPath>\r\n",
       " </defs>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scatter plot the raw data\n",
    "df = pd.concat([lm.train_x, lm.train_y], axis=1)\n",
    "groups = df.groupby(\"y\")\n",
    "for name, group in groups:\n",
    "    plt.plot(group[\"x1\"], group[\"x2\"], marker=\"o\", linestyle=\"\", label=name)\n",
    "    \n",
    "# plot the decision boundary on top of the scattered points\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "x = np.array([-2, 2])\n",
    "y = (beta[0] + beta[1] * x) / -beta[2]\n",
    "\n",
    "plt.plot(x, y)\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Homework 1 :)\n",
    "After you've finished the homework, please print out the entire `ipynb` notebook and two `py` files into one PDF file. Make sure you include the output of code cells and answers for questions. Prepare submit it to GradeScope. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
